{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Building a RAG-Based Researcher Chatbot with LangChain <span style=\"color:orange\">**[75 marks]**</span>\n",
    "\n",
    "Now that you have worked with some of the fundamental components of langchain, you will be tasked with building a somewhat more complex chatbot. This chatbot will be your own personal research assistant. It will have access to a collection of research papers and the ability to answer specific questions according to the contents of those papers. Furthermore, the chatbot will also be able to retrieve information from Wikipedia to formulate a response if the user requests so. To standardise marking, a folder named `papers` containing the research papers you should run your chatbot on has been provided to you.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Instructions\n",
    "\n",
    "- Run the entire notebook to ensure everything is working correctly.\n",
    "- Modify the chatbot's prompt template(s) to suit your specific use cases (e.g., a different context or more detailed instructions).\n",
    "- Do not use GPT or other AI tools to generate code. Refer to documentations instead. It is important you learn these tools yourself for your course projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries\n",
    "\n",
    "Import all of the required libraries here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python installation\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import langchain \n",
    "import langchain_community\n",
    "import langchain_huggingface\n",
    "import langchain_pinecone \n",
    "import pinecone\n",
    "import dotenv\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up API Keys\n",
    "\n",
    "Run the following cell to save any additional API keys into the `.env` file. You may edit this file manually instead if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables are saved to .env file.\n"
     ]
    }
   ],
   "source": [
    "# Replace with the API keys you need\n",
    "API_KEY = \"enter_your_api_key\"\n",
    "\n",
    "env_content = f\"\"\"\n",
    "API_KEY={API_KEY}\n",
    "\"\"\"\n",
    "\n",
    "with open(\".env\", \"w\") as file:\n",
    "    file.write(env_content)\n",
    "\n",
    "print(\"Environment variables are saved to .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Environment File\n",
    "\n",
    "Run the following snippet of code to load the environment file each time you use this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ResearchChatbot Class\n",
    "\n",
    "This is the class where you will be implementing the chatbot. It contains the following methods:\n",
    "\n",
    "- `__init__(self)`: Here you will initialize the chatbot and declare all the required variables/objects, such as the loader, retrievers, llm, prompt templates, etc. <span style=\"color:red\">**[5 marks]**</span>\n",
    "\n",
    "- `load_papers(self)`: Here you will load all the research paper documents. <span style=\"color:red\">**[5 marks]**</span>\n",
    "\n",
    "- `store_papers(self)`: Here you will connect to the vector database, initialize the vector store and store all the embeddings along with their metadata into the database. <span style=\"color:red\">**[5 marks]**</span>\n",
    "\n",
    "- `generate(self, query)`: The user will call this function to generate a response to their query. Here you will invoke the necessary chains and dynamically route the logic further based on the user input. You may assume that the user will make it explicitly clear in their question if they want the chatbot to retrieve a certain Wikipedia article to answer their question. The chatbot should be able to answer questions from the research papers otherwise and must also be capable of responding to general queries. <span style=\"color:red\">**[10 + 10 + 10 marks]**</span>\n",
    "\n",
    "__Some things to note:__\n",
    "1. You may assume that the user will use the chatbot properly. `load_papers` and `store_papers` will always be called at least once before `generate`.\n",
    "\n",
    "2. The chatbot should maintain memory/history of the previous user prompts and responses, thus allowing for follow-up questions to be asked. <span style=\"color:red\">**[10 marks]**</span>\n",
    "\n",
    "3. The chatbot should also be capable of caching previous prompts and responses. If the chatbot recieves a prompt it has seen before or something close to it, then inference should not happen and the chatbot should reuse the prior response, thus improving response times. <span style=\"color:red\">**[10 marks]**</span>\n",
    "\n",
    "4. You can only use langchain components but you may create as many chains, retrievers, prompt templates, etc as you want.\n",
    "\n",
    "5. You may use a different document loader or LLM if you prefer.\n",
    "\n",
    "6. You may create as many helper functions as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement hugging_facehub (from versions: none)\n",
      "ERROR: No matching distribution found for hugging_facehub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting beautifulsoup4 (from wikipedia)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in e:\\python installation\\lib\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\python installation\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\python installation\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\python installation\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\python installation\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.7.4)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->wikipedia)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11704 sha256=4a21c7c8604019bef89ae32a1131d25fd210d2961a5fab58497f896900e49f55\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\63\\47\\7c\\a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: soupsieve, beautifulsoup4, wikipedia\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.6 wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install hugging_facehub\n",
    "%pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Downloading pypdf-5.0.1-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in e:\\python installation\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in e:\\python installation\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in e:\\python installation\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in e:\\python installation\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.8 in e:\\python installation\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in e:\\python installation\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in e:\\python installation\\lib\\site-packages (from langchain) (0.1.131)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in e:\\python installation\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in e:\\python installation\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in e:\\python installation\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in e:\\python installation\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\python installation\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\python installation\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\python installation\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\python installation\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in e:\\python installation\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in e:\\python installation\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in e:\\python installation\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in e:\\python installation\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\python installation\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in e:\\python installation\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in e:\\python installation\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\python installation\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in e:\\python installation\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\python installation\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\python installation\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\python installation\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\python installation\\lib\\site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in e:\\python installation\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in e:\\python installation\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\python installation\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in e:\\python installation\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\python installation\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\python installation\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.8->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import pickle\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from uuid import uuid4\n",
    "from langchain.schema import Document\n",
    "from pinecone import Index\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in e:\\python installation\\lib\\site-packages (0.3.2)\n",
      "Collecting wikipedia-api\n",
      "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: PyYAML>=5.3 in e:\\python installation\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in e:\\python installation\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in e:\\python installation\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.8 in e:\\python installation\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in e:\\python installation\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in e:\\python installation\\lib\\site-packages (from langchain) (0.1.131)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in e:\\python installation\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in e:\\python installation\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in e:\\python installation\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in e:\\python installation\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\python installation\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\python installation\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\python installation\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\python installation\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in e:\\python installation\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in e:\\python installation\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in e:\\python installation\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in e:\\python installation\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\python installation\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in e:\\python installation\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in e:\\python installation\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\python installation\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in e:\\python installation\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\python installation\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\python installation\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\python installation\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\python installation\\lib\\site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in e:\\python installation\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in e:\\python installation\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\python installation\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in e:\\python installation\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\python installation\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\python installation\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.8->langchain) (3.0.0)\n",
      "Building wheels for collected packages: wikipedia-api\n",
      "  Building wheel for wikipedia-api (setup.py): started\n",
      "  Building wheel for wikipedia-api (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14398 sha256=e816a234e34cab4a93dd9593d4f4d2113431004d9a0e3b6886605469ee782388\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\48\\93\\2f\\978da1e445cf17606445f4b47fd8454250f5440d5a10c677e9\n",
      "Successfully built wikipedia-api\n",
      "Installing collected packages: wikipedia-api\n",
      "Successfully installed wikipedia-api-0.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain wikipedia-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchChatbot:\n",
    "\n",
    "    def __init__(self):\n",
    "       \n",
    "        self.pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "        self.huggingface_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "        repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "        self.llm = HuggingFaceEndpoint(\n",
    "            repo_id=repo_id,\n",
    "            temperature= 0.8,\n",
    "            top_k= 25,\n",
    "            huggingfacehub_api_token= self.huggingface_api_key,\n",
    "            \n",
    "            \n",
    "        )\n",
    "        self.wiki_retriever = WikipediaRetriever()\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key =\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name = \"sentence-transformers/all-MiniLM-L6-V2\"\n",
    "        )\n",
    "        self.pc = Pinecone(api_key=self.pinecone_api_key)\n",
    "        #self.cache_file = \"response_cache.pkl\"\n",
    "        #self.load_cache()\n",
    "        self.index_name = \"research-papers\"\n",
    "        try : \n",
    "            existing_indices = [index.name for index in self.pc.list_indexes()]\n",
    "            print(f\"existing indexes: {existing_indices}\")\n",
    "            # index_name_to_delete = 'research-papers'\n",
    "            # if index_name_to_delete in existing_indices:\n",
    "            #     try:\n",
    "            #         self.pc.delete_index(index_name_to_delete)\n",
    "            #         print(f\"Index '{index_name_to_delete}' deleted successfully.\")\n",
    "            #     except Exception as e:\n",
    "            #         print(f\"Error deleting index '{index_name_to_delete}': {e}\")\n",
    "\n",
    "            if self.index_name not in existing_indices:\n",
    "                print(f\"Creating new index: {self.index_name}\")\n",
    "                self.pc.create_index(\n",
    "                    name=self.index_name,\n",
    "                    dimension=384,\n",
    "                    metric=\"cosine\",\n",
    "                    spec=ServerlessSpec(\n",
    "                        cloud=\"aws\",\n",
    "                        region=\"us-east-1\"\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                print(\"created successfully\")\n",
    "            print(\"vector store CREATING\")\n",
    "            self.index = self.pc.Index(self.index_name)\n",
    "            self.vectorstore = PineconeVectorStore(index=self.index, embedding=self.embeddings)\n",
    "            print(\"vector store CREATED\")\n",
    "        except Exception as e : \n",
    "            print(f\"Error in creating pinecone index : {e}\")\n",
    "\n",
    "    \n",
    "\n",
    "    def load_papers(self, folder_path):\n",
    "        \n",
    "        self.pdf_documents = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    pdf_loader = PyPDFLoader(pdf_path)\n",
    "                    document = pdf_loader.load()  \n",
    "                    self.pdf_documents.append(document)\n",
    "                    print(f\"Loaded document: {filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading document {filename}: {e}\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200\n",
    "            )\n",
    "        self.split_documents = []\n",
    "        for doc in self.pdf_documents:\n",
    "            doc_text = \" \".join([chunk.page_content for chunk in doc])\n",
    "            self.split_chunks = text_splitter.split_text(doc_text)  \n",
    "            for chunk in self.split_chunks:\n",
    "                self.split_documents.append(Document(page_content=chunk, metadata={\"source\": filename}))\n",
    "\n",
    "        print(f\"Split into {len(self.split_documents)} chunks.\")\n",
    "    \n",
    "    \n",
    "    def store_papers(self):\n",
    "\n",
    "        print(\"Storing the documents in the pinecone database\")\n",
    "        uuids = [str(uuid4()) for _ in range(len(self.split_documents))]\n",
    "        try: \n",
    "            self.vectorstore.add_documents(documents=self.split_documents, ids=uuids)\n",
    "            print(\"Success\")\n",
    "        except Exception as e : \n",
    "            raise RuntimeError(f\"Error occurred: {str(e)}\")\n",
    "    \n",
    "    def generate(self, query):\n",
    "        try:\n",
    "            # Handling Wikipedia queries\n",
    "            if \"wikipedia\" in query.lower():\n",
    "                docs = WikipediaLoader(query=query, load_max_docs=1).load()\n",
    "                context = docs[0].page_content[:300] if docs else \"No relevant Wikipedia information found.\"\n",
    "\n",
    "                prompt = f\"\"\"You are a helpful research assistant. Use the following Wikipedia information to answer the question. If you can't answer based on the given information, say so.\n",
    "\n",
    "                Wikipedia information: {context}\n",
    "                Question: {query}\n",
    "                Previous conversation: {self.memory.load_memory_variables({\"chat_history\"})}\n",
    "                Assistant:\"\"\"\n",
    "\n",
    "                response = self.llm(prompt)\n",
    "            else:\n",
    "                retriever = self.vectorstore.as_retriever()\n",
    "\n",
    "                qa_prompt_template = \"\"\"You are a helpful research assistant. Use the following pieces of research paper context to answer the question. If you can't answer based on the given context, say so.\n",
    "\n",
    "                Context: {context}\n",
    "\n",
    "                Question: {question}\n",
    "\n",
    "                Previous conversation:\n",
    "                {chat_history}\n",
    "\n",
    "                Assistant:\"\"\"\n",
    "\n",
    "                QA_PROMPT = PromptTemplate(\n",
    "                    template=qa_prompt_template,\n",
    "                    input_variables=[\"context\", \"question\", \"chat_history\"]\n",
    "                )\n",
    "\n",
    "                qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "                    llm=self.llm,\n",
    "                    retriever=retriever,\n",
    "                    memory=self.memory,\n",
    "                    combine_docs_chain_kwargs={\"prompt\": QA_PROMPT}\n",
    "                )\n",
    "\n",
    "                response = qa_chain.invoke({\"question\": query})[\"answer\"]\n",
    "\n",
    "            # Explicitly save context between queries\n",
    "            inputs = {\"question\": query}\n",
    "            outputs = {\"response\": response}\n",
    "            self.memory.save_context(inputs, outputs)  # Saving context (question and response) in memory\n",
    "\n",
    "            # Cache the response\n",
    "            \n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred while generating the response: {str(e)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing your ResearchChatbot\n",
    "\n",
    "Create an instance of your chatbot in the cell below. Showcase all the functionalities of your chatbot below by asking it various types of questions and printing the generated responses. <span style=\"color:red\">**[5 marks]**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\hp\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python installation\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing indexes: ['handbook-chatbot', 'research-papers-2', 'research-papers-4', 'research-papers-5', 'research-papers']\n",
      "vector store CREATING\n",
      "vector store CREATED\n",
      "Loaded document: Can_Foundation_Models_Perform_Zero-Shot_Task_Specification_For_Robot_Manipulation.pdf\n",
      "Loaded document: Domain-Specific_Language_Model_Pretraining_for_Biomedical_Natural_Language_Processing.pdf\n",
      "Loaded document: Improving_Vision-Inspired_Keyword_Spotting_Using_Dynamic_Module_Skipping_in_Streaming_Conformer_Encoder.pdf\n",
      "Loaded document: Large_Language_Models_Understand_and_Can_Be_Enhanced_by_Emotional_Stimuli.pdf\n",
      "Split into 338 chunks.\n",
      "Storing the documents in the pinecone database\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "bot = ResearchChatbot()\n",
    "bot.load_papers(\"./papers\")\n",
    "bot.store_papers()\n",
    "\n",
    "query = \"What is going on?\"\n",
    "#response = bot.generate(query)\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Transformer architecture is a state-of-the-art choice for Automatic Speech Recognition (ASR). It uses a conformer architecture, which includes residual connections in a way that allows gates to dynamically skip modules. Recent work by Peng et al. [18] has shown that binary gates can be added inside a Transformer-based ASR architecture to dynamically adjust the network's depth and reduce the average number of computations while retaining the same word error-rate. They extend the input-dependent dynamic depth (I3D) method to the conformer by placing local gates to skip feedforward, convolution and attention modules based on characteristics of the input to the module itself. Skipping modules still requires the full model to be loaded and does not noticeably impact the user-perceived latency, nevertheless, it improves the efficiency and can lead to considerable power savings.\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain transformer architecture\"\n",
    "response = bot.generate(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Transformer architecture is a state-of-the-art choice for Automatic Speech Recognition (ASR). It uses a conformer architecture, which includes residual connections in a way that allows gates to dynamically skip modules. Recent work by Peng et al. [18] has shown that binary gates can be added inside a Transformer-based ASR architecture to dynamically adjust the network's depth and reduce the average number of computations while retaining the same word error-rate. They extend the input-dependent dynamic depth (I3D) method to the conformer by placing local gates to skip feedforward, convolution and attention modules based on characteristics of the input to the module itself. Skipping modules still requires the full model to be loaded and does not noticeably impact the user-perceived latency, nevertheless, it improves the efficiency and can lead to considerable power savings.\n",
      "\n",
      "The term \"dog whistle\" is a political term, not related to the Transformer architecture or Wikipedia. It seems there might be some confusion in the question or the information provided. If you're asking about the Transformer architecture in the context of ASR, I can provide more specific information. \n",
      "\n",
      "The Transformer architecture is a type of model used in machine learning, particularly in the field of natural language processing (NLP). It was introduced in a paper by Vaswani et al. in 2017 titled \"Attention is All You Need\". The Transformer model dispenses with recurrence and instead relies entirely on self-attention (also called scaled dot-product attention) to draw global dependencies between input and output. It has been a state-of-the-art choice for various NLP tasks, including ASR, due to its ability to handle long-range dependencies and its parallelization capabilities, which can process all input sequences simultaneously.\n",
      "\n",
      "The conformer architecture, which is mentioned in the information you provided, is a type of Transformer model that combines convolution and Transformer-based models. It was introduced in a paper by Gulati et al. in 2020 titled \"Conformer: Convolution-augmented Transformer for Speech Recognition\". The conformer uses a convolution module to capture local dependencies, which is then followed by a Transformer module to capture global dependencies. This architecture has been shown to be particularly effective for ASR tasks.\n",
      "\n",
      "The part\n"
     ]
    }
   ],
   "source": [
    "query = \"what is dog wikipedia\"\n",
    "response = bot.generate(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context, the question appears to be unrelated to the content. The context discusses the use of domain-specific pretraining for natural language processing in the biomedical field, specifically the training of a language model named PubMedBERT. The model is trained on a large corpus of biomedical literature to improve its performance in downstream NLP tasks related to this field. The paper discusses the effectiveness of this approach compared to traditional pretraining methods, as well as potential future directions for this research.\n",
      "\n",
      "Given this, I would respond to the question \"What is a dog?\" by saying that a dog is a common domestic mammal that is often kept as a pet. It is a member of the Canidae family, which also includes wolves, foxes, and other types of animals. Dogs are known for their loyalty and their ability to be trained, and they come in a wide variety of breeds with different characteristics and appearances.\n",
      "\n",
      "However, I want to note that this answer is not related to the provided context, and it is based on general knowledge rather than the content of the text.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is a dog\"\n",
    "response = bot.generate(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The Transformer architecture is a type of model used in machine learning, specifically for Automatic Speech Recognition (ASR). It uses a conformer architecture, which includes residual connections in a way that allows gates to dynamically skip modules. Recent work by Peng et al. [18] has shown that binary gates can be added inside a Transformer-based ASR architecture to dynamically adjust the network's depth and reduce the average number of computations while retaining the same word error-rate. They extend the input-dependent dynamic depth (I3D) method to the conformer by placing local gates to skip feedforward, convolution and attention modules based on characteristics of the input to the module itself. Skipping modules still requires the full model to be loaded and does not noticeably impact the user-perceived latency, nevertheless, it improves the efficiency and can lead to considerable power savings.\n",
      "\n",
      "The Transformer architecture is a state-of-the-art choice for ASR. It uses a conformer architecture, which includes residual connections in a way that allows gates to dynamically skip modules. Recent work by Peng et al. [18] has shown that binary gates can be added inside a Transformer-based ASR architecture to dynamically adjust the network's depth and reduce the average number of computations while retaining the same word error-rate. They extend the input-dependent dynamic depth (I3D) method to the conformer by placing local gates to skip feedforward, convolution and attention modules based on characteristics of the input to the module itself. Skipping modules still requires the full model to be loaded and does not noticeably impact the user-perceived latency, nevertheless, it improves the efficiency and can lead to considerable power savings.\n",
      "\n",
      "The Transformer architecture is a type of model used in machine learning, specifically for Automatic Speech Recognition (ASR). It uses a conformer architecture, which includes residual connections in a way that allows gates to dynamically skip modules. The Transformer architecture is a type of model used in machine learning, specifically in the field of Natural Language Processing (NLP), the Transformer architecture is a type of model used in machine learning, particularly for Automatic Speech Recognition (ASR). It is a state-of-the-art choice for ASR that uses a conformer architecture, which includes residual connections in a way that allows\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain transformer architecture\"\n",
    "response = bot.generate(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a User Interface with Streamlit\n",
    "\n",
    "While you are working on your chatbot, you may use the provided code in `<RollNumber>_interface.py` in order to interact with your research assistant chatbot in a typical conversational frontend. To do so, copy the `ResarchChatbot` class in it's current state to `<RollNumber>_interface.py`, navigate to the directory in a terminal and run `python -m streamlit run <RollNumber>_interface.py`. Streamlit will open an interface for your chatbot in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have taken a dive into learning how to use LangChain to make RAG ChatBots, give yourself a pat on the back. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
